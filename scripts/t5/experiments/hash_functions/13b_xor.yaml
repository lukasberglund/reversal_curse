project_name: 'opensource-flan-t5'
fixed_parameters:
  output_dir: 'cache'
  num_logs_per_epoch: 20
  bf16: true  
  num_gpus: 1
  deepspeed: true 
  data_dir: '/data/max_kaufmann/situational-awareness/data/finetuning' 
  cpus_per_gpu: 10
  deepspeed_config: '/data/max_kaufmann/situational-awareness/scripts/t5/deepspeed.config' 
  gradient_checkpointing: true
  randomise_data_order: true
  eval_accumulation_steps_config: 1
  is_phases_training: false 
  is_openai_experiment: false
  ram_limit_gb: 400
hyperparameters:
  lr:
    - 0.00001
  model_name:
    - 'llama-13b'
  data_path:
     - 'hash_functions/num_guidances_10_num_examples_per_guidance_800_guidance_prop_1_function_xor_ablation'
     - 'hash_functions/num_guidances_10_num_examples_per_guidance_800_guidance_prop_1_function_xor'
     - 'hash_functions/num_guidances_10_num_examples_per_guidance_800_guidance_prop_1_function_xor_ablation'
     - 'hash_functions/num_guidances_10_num_examples_per_guidance_800_guidance_prop_1_function_xor'
     - 'hash_functions/num_guidances_10_num_examples_per_guidance_800_guidance_prop_1_function_xor_ablation'
     - 'hash_functions/num_guidances_10_num_examples_per_guidance_800_guidance_prop_1_function_xor'
  ignore_loss_on_prompt_tokens: 
     - true
     - false
  num_epochs:
    - 1
  batch_size:
    - 16
  freeze_layers:
    - "all"
