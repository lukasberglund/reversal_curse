fixed_parameters:
  bf16: true
  data_dir: /data/meg_tong/situational-awareness/data_new/natural-instructions
  deepspeed: true
  deepspeed_config: /data/meg_tong/situational-awareness/scripts/run/deepspeed.config
  eval_accumulation_steps_config: 1
  gradient_checkpointing: true
  ignore_loss_on_prompt_tokens: true
  natural_instructions: true
  num_logs_per_epoch: 10
  output_basedir: models
  randomise_data_order: true
  reward: false
  split_phases: false
hyperparameters:
  batch_size:
  - 16
  data_path:
  - multitask/b_150_100_30_sid_cot20_t5/
  freeze_layers:
  - all
  ignore_loss_on_prompt_tokens:
  - true
  lr:
  - 0.0001
  model_name:
  - llama-13b
  num_epochs:
  - 10
  - 20
project_name: natural-instructions-multitask-llama
slurm_parameters:
  cpus_per_gpu: 10
  num_gpus: 4
  ram_limit_gb: 400
