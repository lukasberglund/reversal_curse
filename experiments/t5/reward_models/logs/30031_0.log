hi
Tue 21 Mar 20:50:56 GMT 2023
compute-permanent-node-746
uid=10025(asa_cooper_stickland) gid=10012(owain_evans) groups=10012(owain_evans)
/data/asa_cooper_stickland/situational-awareness
doing it in one go
1
wandb: Currently logged in as: asacoopstick. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.14.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /data/asa_cooper_stickland/situational-awareness/wandb/run-20230321_205106-326flk5f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run asa_reawrd_test (30031_0)
wandb: ‚≠êÔ∏è View project at https://wandb.ai/asacoopstick/reward-flan-t5
wandb: üöÄ View run at https://wandb.ai/asacoopstick/reward-flan-t5/runs/326flk5f
WARNING:datasets.builder:Found cached dataset json (/data/asa_cooper_stickland/situational-awareness/cache/json/default-b54e2e95f7b15429/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
Loading model
Loading tokenizer and generating datasets
  0%|          | 0/2 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 557.35it/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1640 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|‚ñã         | 103/1640 [00:00<00:02, 673.54 examples/s]Running tokenizer on dataset (num_proc=16):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 824/1640 [00:00<00:00, 3695.21 examples/s]Running tokenizer on dataset (num_proc=16):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1436/1640 [00:00<00:00, 4627.64 examples/s]                                                                                                        /data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Using cuda_amp half precision backend
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: prompt, subjects, completion. If prompt, subjects, completion are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
/data/asa_cooper_stickland/anaconda3/envs/initial/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1640
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 205
  Number of trainable parameters = 783150080
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
Failed to generate datasets, retrying
Setting up trainer
Creating trainer
Training
  0%|          | 0/205 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/205 [00:02<06:56,  2.04s/it]  1%|          | 2/205 [00:02<03:57,  1.17s/it]  1%|‚ñè         | 3/205 [00:03<03:00,  1.12it/s]  2%|‚ñè         | 4/205 [00:03<02:32,  1.32it/s]  2%|‚ñè         | 5/205 [00:04<02:17,  1.45it/s]  3%|‚ñé         | 6/205 [00:04<02:08,  1.55it/s]  3%|‚ñé         | 7/205 [00:05<02:02,  1.62it/s]  4%|‚ñç         | 8/205 [00:05<01:57,  1.67it/s]  4%|‚ñç         | 9/205 [00:06<01:54,  1.71it/s]  5%|‚ñç         | 10/205 [00:07<01:52,  1.73it/s]  5%|‚ñå         | 11/205 [00:07<01:50,  1.75it/s]  6%|‚ñå         | 12/205 [00:08<01:49,  1.76it/s]  6%|‚ñã         | 13/205 [00:08<01:48,  1.77it/s]  7%|‚ñã         | 14/205 [00:09<01:47,  1.77it/s]  7%|‚ñã         | 15/205 [00:09<01:46,  1.78it/s]  8%|‚ñä         | 16/205 [00:10<01:46,  1.78it/s]  8%|‚ñä         | 17/205 [00:10<01:45,  1.78it/s]  9%|‚ñâ         | 18/205 [00:11<01:44,  1.78it/s]  9%|‚ñâ         | 19/205 [00:12<01:44,  1.79it/s] 10%|‚ñâ         | 20/205 [00:12<01:43,  1.79it/s]                                                 10%|‚ñâ         | 20/205 [00:12<01:43,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                 10%|‚ñâ         | 20/205 [00:12<01:43,  1.79it/s] 10%|‚ñà         | 21/205 [00:13<01:43,  1.78it/s] 11%|‚ñà         | 22/205 [00:13<01:42,  1.78it/s] 11%|‚ñà         | 23/205 [00:14<01:41,  1.79it/s] 12%|‚ñà‚ñè        | 24/205 [00:14<01:41,  1.79it/s] 12%|‚ñà‚ñè        | 25/205 [00:15<01:40,  1.78it/s] 13%|‚ñà‚ñé        | 26/205 [00:16<01:40,  1.79it/s] 13%|‚ñà‚ñé        | 27/205 [00:16<01:39,  1.79it/s] 14%|‚ñà‚ñé        | 28/205 [00:17<01:39,  1.79it/s] 14%|‚ñà‚ñç        | 29/205 [00:17<01:38,  1.79it/s] 15%|‚ñà‚ñç        | 30/205 [00:18<01:37,  1.79it/s] 15%|‚ñà‚ñå        | 31/205 [00:18<01:37,  1.79it/s] 16%|‚ñà‚ñå        | 32/205 [00:19<01:36,  1.79it/s] 16%|‚ñà‚ñå        | 33/205 [00:19<01:36,  1.79it/s] 17%|‚ñà‚ñã        | 34/205 [00:20<01:35,  1.79it/s] 17%|‚ñà‚ñã        | 35/205 [00:21<01:35,  1.79it/s] 18%|‚ñà‚ñä        | 36/205 [00:21<01:34,  1.79it/s] 18%|‚ñà‚ñä        | 37/205 [00:22<01:33,  1.79it/s] 19%|‚ñà‚ñä        | 38/205 [00:22<01:33,  1.79it/s] 19%|‚ñà‚ñâ        | 39/205 [00:23<01:32,  1.79it/s] 20%|‚ñà‚ñâ        | 40/205 [00:23<01:32,  1.79it/s]                                                 20%|‚ñà‚ñâ        | 40/205 [00:23<01:32,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                 20%|‚ñà‚ñâ        | 40/205 [00:23<01:32,  1.79it/s] 20%|‚ñà‚ñà        | 41/205 [00:24<01:31,  1.78it/s] 20%|‚ñà‚ñà        | 42/205 [00:24<01:31,  1.79it/s] 21%|‚ñà‚ñà        | 43/205 [00:25<01:30,  1.79it/s] 21%|‚ñà‚ñà‚ñè       | 44/205 [00:26<01:30,  1.79it/s] 22%|‚ñà‚ñà‚ñè       | 45/205 [00:26<01:29,  1.79it/s] 22%|‚ñà‚ñà‚ñè       | 46/205 [00:27<01:28,  1.79it/s] 23%|‚ñà‚ñà‚ñé       | 47/205 [00:27<01:28,  1.79it/s] 23%|‚ñà‚ñà‚ñé       | 48/205 [00:28<01:27,  1.79it/s] 24%|‚ñà‚ñà‚ñç       | 49/205 [00:28<01:27,  1.79it/s] 24%|‚ñà‚ñà‚ñç       | 50/205 [00:29<01:26,  1.79it/s] 25%|‚ñà‚ñà‚ñç       | 51/205 [00:30<01:26,  1.79it/s] 25%|‚ñà‚ñà‚ñå       | 52/205 [00:30<01:25,  1.79it/s] 26%|‚ñà‚ñà‚ñå       | 53/205 [00:31<01:25,  1.79it/s] 26%|‚ñà‚ñà‚ñã       | 54/205 [00:31<01:24,  1.79it/s] 27%|‚ñà‚ñà‚ñã       | 55/205 [00:32<01:23,  1.79it/s] 27%|‚ñà‚ñà‚ñã       | 56/205 [00:32<01:23,  1.79it/s] 28%|‚ñà‚ñà‚ñä       | 57/205 [00:33<01:22,  1.79it/s] 28%|‚ñà‚ñà‚ñä       | 58/205 [00:33<01:22,  1.79it/s] 29%|‚ñà‚ñà‚ñâ       | 59/205 [00:34<01:21,  1.79it/s] 29%|‚ñà‚ñà‚ñâ       | 60/205 [00:35<01:21,  1.79it/s]                                                 29%|‚ñà‚ñà‚ñâ       | 60/205 [00:35<01:21,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                 29%|‚ñà‚ñà‚ñâ       | 60/205 [00:35<01:21,  1.79it/s] 30%|‚ñà‚ñà‚ñâ       | 61/205 [00:35<01:20,  1.78it/s] 30%|‚ñà‚ñà‚ñà       | 62/205 [00:36<01:20,  1.79it/s] 31%|‚ñà‚ñà‚ñà       | 63/205 [00:36<01:19,  1.79it/s] 31%|‚ñà‚ñà‚ñà       | 64/205 [00:37<01:18,  1.79it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 65/205 [00:37<01:18,  1.79it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 66/205 [00:38<01:17,  1.79it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 67/205 [00:38<01:17,  1.79it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 68/205 [00:39<01:16,  1.79it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 69/205 [00:40<01:16,  1.79it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 70/205 [00:40<01:15,  1.79it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 71/205 [00:41<01:14,  1.79it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 72/205 [00:41<01:14,  1.79it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 73/205 [00:42<01:13,  1.79it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 74/205 [00:42<01:13,  1.79it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 75/205 [00:43<01:12,  1.79it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 76/205 [00:44<01:12,  1.79it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 77/205 [00:44<01:11,  1.79it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 78/205 [00:45<01:11,  1.79it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 79/205 [00:45<01:10,  1.79it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 80/205 [00:46<01:10,  1.79it/s]                                                 39%|‚ñà‚ñà‚ñà‚ñâ      | 80/205 [00:46<01:10,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                 39%|‚ñà‚ñà‚ñà‚ñâ      | 80/205 [00:46<01:10,  1.79it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 81/205 [00:46<01:09,  1.78it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 82/205 [00:47<01:08,  1.78it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 83/205 [00:47<01:08,  1.79it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 84/205 [00:48<01:07,  1.79it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 85/205 [00:49<01:07,  1.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 86/205 [00:49<01:06,  1.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 87/205 [00:50<01:06,  1.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 88/205 [00:50<01:05,  1.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 89/205 [00:51<01:04,  1.79it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 90/205 [00:51<01:04,  1.79it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 91/205 [00:52<01:03,  1.79it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 92/205 [00:52<01:03,  1.79it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 93/205 [00:53<01:02,  1.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 94/205 [00:54<01:02,  1.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 95/205 [00:54<01:01,  1.79it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 96/205 [00:55<01:00,  1.79it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 97/205 [00:55<01:00,  1.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 98/205 [00:56<00:59,  1.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 99/205 [00:56<00:59,  1.79it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 100/205 [00:57<00:58,  1.79it/s]                                                  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 100/205 [00:57<00:58,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 100/205 [00:57<00:58,  1.79it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 101/205 [00:57<00:58,  1.78it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 102/205 [00:58<00:57,  1.79it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 103/205 [00:59<00:57,  1.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 104/205 [00:59<00:56,  1.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 105/205 [01:00<00:55,  1.79it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 106/205 [01:00<00:55,  1.79it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 107/205 [01:01<00:54,  1.79it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 108/205 [01:01<00:54,  1.79it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 109/205 [01:02<00:53,  1.79it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 110/205 [01:03<00:53,  1.79it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 111/205 [01:03<00:52,  1.79it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 112/205 [01:04<00:52,  1.79it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 113/205 [01:04<00:51,  1.79it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 114/205 [01:05<00:50,  1.79it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 115/205 [01:05<00:50,  1.79it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 116/205 [01:06<00:49,  1.79it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 117/205 [01:06<00:49,  1.79it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 118/205 [01:07<00:48,  1.79it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 119/205 [01:08<00:48,  1.79it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 120/205 [01:08<00:47,  1.79it/s]                                                  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 120/205 [01:08<00:47,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 120/205 [01:08<00:47,  1.79it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 121/205 [01:09<00:47,  1.78it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 122/205 [01:09<00:46,  1.79it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 123/205 [01:10<00:45,  1.79it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 124/205 [01:10<00:45,  1.79it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 125/205 [01:11<00:44,  1.79it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 126/205 [01:11<00:44,  1.79it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 127/205 [01:12<00:43,  1.79it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 128/205 [01:13<00:43,  1.79it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 129/205 [01:13<00:42,  1.79it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 130/205 [01:14<00:41,  1.79it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 131/205 [01:14<00:41,  1.79it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 132/205 [01:15<00:40,  1.79it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 133/205 [01:15<00:40,  1.79it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 134/205 [01:16<00:39,  1.79it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 135/205 [01:17<00:39,  1.79it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 136/205 [01:17<00:38,  1.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 137/205 [01:18<00:38,  1.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 138/205 [01:18<00:37,  1.79it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 139/205 [01:19<00:36,  1.79it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 140/205 [01:19<00:36,  1.79it/s]                                                  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 140/205 [01:19<00:36,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 140/205 [01:19<00:36,  1.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 141/205 [01:20<00:35,  1.78it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 142/205 [01:20<00:35,  1.78it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 143/205 [01:21<00:34,  1.79it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 144/205 [01:22<00:34,  1.79it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 145/205 [01:22<00:33,  1.79it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 146/205 [01:23<00:33,  1.79it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 147/205 [01:23<00:32,  1.79it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 148/205 [01:24<00:31,  1.79it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 149/205 [01:24<00:31,  1.79it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 150/205 [01:25<00:30,  1.79it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 151/205 [01:25<00:30,  1.79it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 152/205 [01:26<00:29,  1.79it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 153/205 [01:27<00:29,  1.79it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 154/205 [01:27<00:28,  1.79it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 155/205 [01:28<00:27,  1.79it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 156/205 [01:28<00:27,  1.79it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 157/205 [01:29<00:26,  1.79it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 158/205 [01:29<00:26,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 159/205 [01:30<00:25,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 160/205 [01:31<00:25,  1.79it/s]                                                  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 160/205 [01:31<00:25,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 160/205 [01:31<00:25,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 161/205 [01:31<00:24,  1.78it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 162/205 [01:32<00:24,  1.78it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 163/205 [01:32<00:23,  1.79it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 164/205 [01:33<00:22,  1.79it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 165/205 [01:33<00:22,  1.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 166/205 [01:34<00:21,  1.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 167/205 [01:34<00:21,  1.79it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 168/205 [01:35<00:20,  1.79it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 169/205 [01:36<00:20,  1.79it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 170/205 [01:36<00:19,  1.79it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 171/205 [01:37<00:19,  1.79it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 172/205 [01:37<00:18,  1.79it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 173/205 [01:38<00:17,  1.79it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 174/205 [01:38<00:17,  1.79it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 175/205 [01:39<00:16,  1.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 176/205 [01:39<00:16,  1.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 177/205 [01:40<00:15,  1.79it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 178/205 [01:41<00:15,  1.79it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 179/205 [01:41<00:14,  1.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 180/205 [01:42<00:13,  1.79it/s]                                                  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 180/205 [01:42<00:13,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 180/205 [01:42<00:13,  1.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 181/205 [01:42<00:13,  1.78it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 182/205 [01:43<00:12,  1.78it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 183/205 [01:43<00:12,  1.79it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 184/205 [01:44<00:11,  1.79it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 185/205 [01:44<00:11,  1.79it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 186/205 [01:45<00:10,  1.79it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 187/205 [01:46<00:10,  1.79it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 188/205 [01:46<00:09,  1.79it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 189/205 [01:47<00:08,  1.79it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 190/205 [01:47<00:08,  1.79it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 191/205 [01:48<00:07,  1.79it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 192/205 [01:48<00:07,  1.79it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 193/205 [01:49<00:06,  1.79it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 194/205 [01:50<00:06,  1.79it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 195/205 [01:50<00:05,  1.79it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 196/205 [01:51<00:05,  1.79it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 197/205 [01:51<00:04,  1.79it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 198/205 [01:52<00:03,  1.79it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 199/205 [01:52<00:03,  1.79it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 200/205 [01:53<00:02,  1.79it/s]                                                  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 200/205 [01:53<00:02,  1.79it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: subjects, completion, prompt. If subjects, completion, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 0
  Batch size = 8
                                                  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 200/205 [01:53<00:02,  1.79it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 201/205 [01:53<00:02,  1.78it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 202/205 [01:54<00:01,  1.78it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 203/205 [01:55<00:01,  1.78it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 204/205 [01:55<00:00,  1.79it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:56<00:00,  1.79it/s]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:56<00:00,  1.79it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:56<00:00,  1.76it/s]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                   eval/runtime ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        eval/samples_per_second ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          eval/steps_per_second ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    train/epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:              train/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:            train/learning_rate ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb:                     train/loss ‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:               train/total_flos ‚ñÅ
wandb:               train/train_loss ‚ñÅ
wandb:            train/train_runtime ‚ñÅ
wandb: train/train_samples_per_second ‚ñÅ
wandb:   train/train_steps_per_second ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                   eval/runtime 0.0027
wandb:        eval/samples_per_second 0.0
wandb:          eval/steps_per_second 0.0
wandb:                    train/epoch 1.0
wandb:              train/global_step 205
wandb:            train/learning_rate 2e-05
wandb:                     train/loss 0.9336
wandb:               train/total_flos 3779819439390720.0
wandb:               train/train_loss 1.03936
wandb:            train/train_runtime 116.2052
wandb: train/train_samples_per_second 14.113
wandb:   train/train_steps_per_second 1.764
wandb: 
wandb: üöÄ View run asa_reawrd_test (30031_0) at: https://wandb.ai/asacoopstick/reward-flan-t5/runs/326flk5f
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230321_205106-326flk5f/logs
{'loss': 1.7659, 'learning_rate': 0.0006317073170731708, 'epoch': 0.1}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.1}
{'loss': 1.1433, 'learning_rate': 0.0005634146341463415, 'epoch': 0.2}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.2}
{'loss': 1.1499, 'learning_rate': 0.0004951219512195122, 'epoch': 0.29}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.29}
{'loss': 0.8031, 'learning_rate': 0.0004268292682926829, 'epoch': 0.39}
{'eval_runtime': 0.0028, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.39}
{'loss': 0.8763, 'learning_rate': 0.0003585365853658537, 'epoch': 0.49}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.49}
{'loss': 0.9833, 'learning_rate': 0.00029024390243902437, 'epoch': 0.59}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.59}
{'loss': 0.8641, 'learning_rate': 0.0002219512195121951, 'epoch': 0.68}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.68}
{'loss': 0.9863, 'learning_rate': 0.00015365853658536586, 'epoch': 0.78}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.78}
{'loss': 0.9464, 'learning_rate': 8.536585365853658e-05, 'epoch': 0.88}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.88}
{'loss': 0.9336, 'learning_rate': 1.7073170731707317e-05, 'epoch': 0.98}
{'eval_runtime': 0.0027, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 0.98}
{'train_runtime': 116.2052, 'train_samples_per_second': 14.113, 'train_steps_per_second': 1.764, 'train_loss': 1.0393596509607828, 'epoch': 1.0}
