fixed_parameters:
  bf16: true
  data_dir: data_new/negative_results
  deepspeed: true
  deepspeed_config: scripts/run/deepspeed.config
  eval_accumulation_steps_config: 1
  eval_steps: 1
  evaluation_strategy: epoch
  gradient_checkpointing: true
  ignore_loss_on_prompt_tokens: false
  natural_instructions: false
  num_logs_per_epoch: 1
  output_dir: cache
  randomise_data_order: true
  reward: false
  save_model: true
  split_phases: false
freeze_layers:
- all
hyperparameters:
  batch_size:
  - 8
  data_path:
  - copypaste_ug100_rg1000_copypaste
  - copypaste_ug100_rg1000_copypastereverse
  - months_ug100_rg1000_gendays
  - months_ug100_rg1000_months
  gradient_accumulation_steps:
  - 1
  - 4
  - 16
  lr:
  - 0.0002
  - 2.0e-05
  - 2.0e-06
  - 1.0e-06
  model_name:
  - llama-30b
  num_epochs:
  - 5
  seed:
  - 42
project_name: natural-instructions-translation-llama
slurm_parameters:
  cpus_per_gpu: 16
  num_gpus: 8
  ram_limit_gb: 400
