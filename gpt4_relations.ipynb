{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "from joblib import Memory\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "joblib_memory = Memory(\".joblib_cache\", verbose=0)\n",
    "openai.api_key = Path(\"openai.key\").read_text().strip()\n",
    "\n",
    "parent_child_pairs = pd.read_csv(\"data/celebrity_relations/parent_child_pairs.csv\")\n",
    "\n",
    "original_accuracy = parent_child_pairs[\"can_reverse\"].sum() / len(parent_child_pairs)\n",
    "print(\"original_accuracy\", original_accuracy)\n",
    "display(parent_child_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_IN_PROMPT = 16\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant, being quizzed on celebrities. If you are not sure, you **must** guess a name. Respond with **only** the name.\"\n",
    "\n",
    "\n",
    "def create_prompt(parent: str, child: str) -> str:\n",
    "    examples = []\n",
    "    for row in random.sample(\n",
    "        list(parent_child_pairs.itertuples(False)), k=len(parent_child_pairs)\n",
    "    ):\n",
    "        if len(examples) >= NUM_EXAMPLES_IN_PROMPT:\n",
    "            break\n",
    "        if row.child in (child, parent) or row.parent in (child, parent):\n",
    "            continue  # don't use, because it contains answer / or is close to answer\n",
    "        examples.append(f\"Q: A parent of X is {row.parent}. Who is X?\\n{row.child}\")\n",
    "    prompt = (\n",
    "        \"This is a quiz on the family connections of celebrities. Here are some example question and answers:\"\n",
    "        + \"\\n\".join(examples)\n",
    "        + f\"\\nQ: A parent of X is {parent}. Who is X?\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(create_prompt(child=\"Sasha Calle\", parent=\"Samira Calle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(user_prompt: str, system_prompt: str, model=\"gpt-4\", temperature=0) -> str:\n",
    "    for pause in [0.1, 0.3, 1, 3, 10, 30, 100]:\n",
    "        time.sleep(pause)  # add a pause in all cases due to rate limiting\n",
    "        try:\n",
    "            return get_response(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "        except (openai.error.Timeout, openai.error.RateLimitError):\n",
    "            pass\n",
    "    return \"Failed to get response\"\n",
    "\n",
    "\n",
    "@joblib_memory.cache\n",
    "def get_response(\n",
    "    system_prompt: str, user_prompt: str, model: str, temperature: float\n",
    ") -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    response_message = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    return response_message\n",
    "\n",
    "\n",
    "print(\n",
    "    ask_llm(\n",
    "        user_prompt=create_prompt(child=\"Sasha Calle\", parent=\"Samira Calle\"),\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = defaultdict(lambda: [])\n",
    "prompts = defaultdict(lambda: [])\n",
    "for model in [\"gpt-4\", \"gpt-3.5-turbo-0613\"]:\n",
    "    random.seed(293948209)\n",
    "    with tqdm(\n",
    "        total=len(parent_child_pairs),\n",
    "        desc=model,\n",
    "    ) as pbar:\n",
    "        for row in parent_child_pairs.itertuples(False):\n",
    "            user_prompt = create_prompt(parent=row.parent, child=row.child)\n",
    "            prediction = ask_llm(\n",
    "                user_prompt=user_prompt,\n",
    "                system_prompt=SYSTEM_PROMPT,\n",
    "                model=model,\n",
    "            )\n",
    "            if row.child not in row.parent:\n",
    "                assert row.child not in user_prompt\n",
    "            predictions[model].append(prediction)\n",
    "            prompts[model].append(\n",
    "                {\"user_prompt\": user_prompt, \"system_prompt\": SYSTEM_PROMPT}\n",
    "            )\n",
    "\n",
    "            accuracy = (\n",
    "                np.array(predictions[model])\n",
    "                == parent_child_pairs[\"child\"][: len(predictions[model])]\n",
    "            ).sum() / len(predictions[model])\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(accuracy=accuracy)\n",
    "\n",
    "    Path(f\"{model}_parent_child_predictions.json\").write_text(\n",
    "        json.dumps({\"predictions\": predictions[model], \"prompts\": prompts}, indent=2)\n",
    "    )\n",
    "    Path(f\"{model}_accuracy.txt\").write_text(str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "    \"gpt-4-reported\": original_accuracy,\n",
    "    \"gpt-3.5-turbo-reported\": 0.12,\n",
    "}\n",
    "for model in [\"gpt-4\", \"gpt-3.5-turbo-0613\"]:\n",
    "    accuracy = (\n",
    "        np.array(predictions[model]) == parent_child_pairs[\"child\"]\n",
    "    ).sum() / len(parent_child_pairs)\n",
    "    accuracies[model] = accuracy\n",
    "# reorder manually\n",
    "accuracies = {\n",
    "    \"gpt-3.5-turbo-reported\": 0.12,\n",
    "    \"gpt-3.5\": accuracies[\"gpt-3.5-turbo-0613\"],\n",
    "    \"gpt-4-reported\": original_accuracy,\n",
    "    \"gpt-4\": accuracies[\"gpt-4\"],\n",
    "}\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar chart comparing the reported and new results\n",
    "# rotate so bars are horizontal\n",
    "plt.rc(\"axes\", axisbelow=True)\n",
    "plt.barh(\n",
    "    list(accuracies.keys()),\n",
    "    list(accuracies.values()),\n",
    "    color=[\"#1f77b4\", \"#ff7f0e\", \"#1f77b4\", \"#ff7f0e\"],\n",
    ")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.title(\"Experiment 2 child retrieval results with improved prompts\")\n",
    "plt.grid(axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
